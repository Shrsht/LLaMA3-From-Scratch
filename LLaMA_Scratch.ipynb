{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM607Mhi6teVGQI+a2CfXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrsht/LLaMA3-From-Scratch/blob/main/LLaMA_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghcwHvb51Dov"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotate with images and explanations:"
      ],
      "metadata": {
        "id": "iiSn_CCnNuL_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-OMj1MsQ1S_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary length. Llama's real vocab size is 128256. Here let's just use an absurdly small number\n",
        "\n",
        "\n",
        "v = 10 ## total number of words in our entire vocabulary\n",
        "\n",
        "# Llama's maximum sequence length is 8192, but for inference they cache 3/4 of it and only use an effective length of 2048. more on that later\n",
        "\n",
        "seq_len = 5  ##number of words in a given sentence/phrase/etc.\n",
        "\n",
        "# we'll use a batch size of 1 for simplicity when visualizing our tensors\n",
        "b = 1 ## training input size\n",
        "\n",
        "# now let's make ourselves a list of token indices. Each represents somewhere between a letter and a word\n",
        "tokens = torch.randint(v, (b, seq_len))\n",
        "tokens.shape, tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEje5kPPeW1Y",
        "outputId": "55fe28b1-afe0-4b07-bb5b-38db0f4edf27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5]), tensor([[1, 0, 6, 0, 2]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding our Vocabulary:\n",
        "\n",
        "***Purpose of Embeddings:***\n",
        "\n",
        "- Need to find a way to represent our input words in a vector-space.\n",
        "- Ideally words with a similar meaning are closer to each other in the vector space"
      ],
      "metadata": {
        "id": "5ejwAdmd45Ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1b. Initializing the first residual state"
      ],
      "metadata": {
        "id": "Bib-slQZ2Pdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our embedding dimension. Llama 3 8b's is 4096\n",
        "d = 16\n",
        "\n",
        "\n",
        "# initializing our token embedding matrix\n",
        "embedding = nn.Embedding(v, d)\n",
        "\n",
        "\n",
        "## for each of our v = 10 words, we embedded the vectors of the tokens into a 16-dimensional space\n",
        "\n",
        "### So there are now 10 vectors, w/ 16-dimensions that are a vector representation of our entire vocabulary\n",
        "\n",
        "embedding.weight.shape, embedding.weight\n",
        "# each row in this embedding is a high dimensional repersentation of its corresponding token\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85sdYS3T2LML",
        "outputId": "909196b9-de68-4bdc-8084-578f50c4d793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 16]),\n",
              " Parameter containing:\n",
              " tensor([[ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,  0.9639,\n",
              "          -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056, -0.3937,  0.5133],\n",
              "         [ 0.7922, -1.0242,  0.1843, -1.0991, -0.9402, -1.9237,  1.4471, -1.6211,\n",
              "          -0.4627,  0.0724, -2.5458,  0.3172,  0.7139,  0.8306, -0.3575, -0.5453],\n",
              "         [ 1.0906, -0.5341,  1.5008, -2.1458, -0.2829,  0.4449, -2.3409, -0.6489,\n",
              "          -0.3310, -0.3682, -0.3702,  1.2048, -0.4190, -0.5210, -0.8396, -1.5450],\n",
              "         [-1.5210, -0.1012, -1.4199, -1.2587,  0.0276,  0.1067, -0.4456, -0.5917,\n",
              "           0.6417, -0.7544,  0.8427, -0.7505,  0.7146, -1.1200, -0.0906,  0.0935],\n",
              "         [-0.3263, -0.5366, -0.1466,  0.9948,  0.7581, -0.3410, -0.7700,  0.3221,\n",
              "           0.7490,  1.5158, -0.8681, -1.2408,  1.0331,  0.0739,  0.3373, -0.6929],\n",
              "         [ 0.0199,  0.6430, -0.6010,  1.3610, -0.8156,  0.6825,  0.5961, -0.9829,\n",
              "          -0.6699, -1.5987,  0.2250,  0.3872,  1.2098, -0.6168, -2.3691,  0.2740],\n",
              "         [ 0.4887, -0.1694,  0.4623,  1.1920,  0.2188, -2.1144, -0.6303, -1.2609,\n",
              "          -0.4754, -0.4203, -0.3314,  0.0310,  0.8525,  0.0888,  0.4840, -0.7512],\n",
              "         [-0.8883,  2.1744,  0.1384,  0.0968,  0.1787, -2.3260, -0.6356, -0.7342,\n",
              "          -0.8303,  0.7244,  0.7029, -1.6178, -0.3832,  0.2671, -1.1235,  0.5351],\n",
              "         [-0.1134, -0.0673, -1.2983,  0.1786, -0.7409,  2.8965,  0.4021,  0.5070,\n",
              "          -0.6776, -0.3954, -1.5393, -0.5944, -0.1295, -1.0192, -0.4519,  0.9672],\n",
              "         [-0.4657,  1.8831,  1.3783,  0.0082, -1.9812,  0.5819,  0.2648,  0.0770,\n",
              "          -0.1659, -1.1301,  3.5666, -0.1952, -1.0927, -0.0058,  1.1573,  0.5783]],\n",
              "        requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We want the embedings of our 5 specific words - so we feed the tokens of these words into the embeddings() object.\n",
        "\n",
        "x = embedding(tokens)\n",
        "x.shape, x\n",
        "\n",
        "# at this points many models would multiply the embeddings by the square root of the embedding dimension, but Llama 3 foregoes that strategy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad8ioLCN2LPJ",
        "outputId": "be20e758-ec71-49e1-fc22-336b91119c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 0.7922, -1.0242,  0.1843, -1.0991, -0.9402, -1.9237,  1.4471,\n",
              "           -1.6211, -0.4627,  0.0724, -2.5458,  0.3172,  0.7139,  0.8306,\n",
              "           -0.3575, -0.5453],\n",
              "          [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
              "            0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
              "           -0.3937,  0.5133],\n",
              "          [ 0.4887, -0.1694,  0.4623,  1.1920,  0.2188, -2.1144, -0.6303,\n",
              "           -1.2609, -0.4754, -0.4203, -0.3314,  0.0310,  0.8525,  0.0888,\n",
              "            0.4840, -0.7512],\n",
              "          [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
              "            0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
              "           -0.3937,  0.5133],\n",
              "          [ 1.0906, -0.5341,  1.5008, -2.1458, -0.2829,  0.4449, -2.3409,\n",
              "           -0.6489, -0.3310, -0.3682, -0.3702,  1.2048, -0.4190, -0.5210,\n",
              "           -0.8396, -1.5450]]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding:\n",
        "\n",
        "\n",
        "by default the attention mechanism is blind to the ordering of tokens - so we need to find a way to represent order of tokens.\n",
        "\n",
        "\n",
        "### RoPE Encoding\n",
        "<a id='c'></a>\n",
        "\n",
        "Rotary Positional Encoding (RoPE) is a method [originally proposed in 2019](https://arxiv.org/abs/2104.09864) that quickly became the defacto standard for enabling transformers to understand positional information.\n",
        "\n",
        "The method utilizes trigonometry to \"rotate\" the entries in two matrices before they are multiplied together. A small amount of rotation indicates that two tokens are close together, while a large amount of rotation corresponds to being far apart.\n",
        "\n",
        "\n",
        "We will precompute these positional encodings because we want to reuse them throughout the model as opposed to creating them from scratch every time we need them."
      ],
      "metadata": {
        "id": "_Sk1TSti4lss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = 10000 # 10,000 is the most common value but Llama 3 uses 50,000. In theory smaller models should use a smaller value\n",
        "num_heads = 4 # Llama 3 8b has 32 total attention heads\n",
        "head_dim = d // num_heads # Llama 3 ties its head dimension to the embedding dimension. This value comes out to 128 in Llama 3, which is purposeful to\n",
        "\n",
        "# go watch the video to get a better explanation of what's happening here\n",
        "freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
        "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
        "\n",
        "t = torch.arange(seq_len * 2, device=freqs.device, dtype=torch.float32)\n",
        "print(f't: {t.shape}\\n{t}\\n')\n",
        "\n",
        "freqs = torch.outer(t, freqs)\n",
        "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
        "\n",
        "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)[:seq_len]  # complex64\n",
        "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ph_Dvcp2LSN",
        "outputId": "3d49a5da-34e2-48bf-e462-4c6591ac8b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freqs: torch.Size([2])\n",
            "tensor([1.0000, 0.0100])\n",
            "\n",
            "t: torch.Size([10])\n",
            "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
            "\n",
            "freqs: torch.Size([10, 2])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [1.0000, 0.0100],\n",
            "        [2.0000, 0.0200],\n",
            "        [3.0000, 0.0300],\n",
            "        [4.0000, 0.0400],\n",
            "        [5.0000, 0.0500],\n",
            "        [6.0000, 0.0600],\n",
            "        [7.0000, 0.0700],\n",
            "        [8.0000, 0.0800],\n",
            "        [9.0000, 0.0900]])\n",
            "\n",
            "freqs_cis: torch.Size([5, 2])\n",
            "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j],\n",
            "        [ 0.5403+0.8415j,  0.9999+0.0100j],\n",
            "        [-0.4161+0.9093j,  0.9998+0.0200j],\n",
            "        [-0.9900+0.1411j,  0.9996+0.0300j],\n",
            "        [-0.6536-0.7568j,  0.9992+0.0400j]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precomputing the Causal Mask\n",
        "<a id='d'></a>\n",
        "\n",
        "\n",
        "The basic idea of a causal mask is that by default, attention mechanisms allow every single token to pay attention to every single other token.\n",
        "\n",
        "This is okay or even preferable for some model types, but Llama is auto-regressive, meaning it would be bad if a given token to be predicted was able to see itself and future tokens during training but not during inference.\n",
        "\n",
        "\n",
        "The negative infinity's in the upper-triangle prevent the model from attending to the corresponding token; how this works will be more clear later when we do the attention softmax"
      ],
      "metadata": {
        "id": "K5ND0Si09abT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.full(\n",
        "    (seq_len, seq_len),\n",
        "    float(\"-inf\") ) ##This code creates a square tensor of size seq_len x seq_len filled entirely with negative infinity values (float(\"-inf\")).)\n",
        "\n",
        "\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "mask\n",
        "\n",
        "### creates -inf in the upper portion of the matrix as a max to prevent future words from being seen before the token is produced:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwLf5G3_9JPh",
        "outputId": "f29079a2-3a4a-4531-bd5b-61b11bd8979a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMS Normalization:\n",
        "\n",
        "\n",
        "We want a kind of normalization that does not refactor the mean.\n",
        "\n",
        "Root Mean Square Normalization has also been the norm for quite awhile. Like its predecessor LayerNorm, RMSNorm restricts the variability of the entries in each embedding vector such that the vector lies on a hypersphere with radius $\\sqrt{d}$. However unlike LayerNorm which centers that hypersphere with a mean of zero, RMSNorm does not mess with the mean, which is an important source of data for networks that utilize residual connections.\n"
      ],
      "metadata": {
        "id": "VCea8V5G95v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUASbq_k_W6I",
        "outputId": "019fee15-0d9e-4543-ec3d-fd5ac08184f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7922, -1.0242,  0.1843, -1.0991, -0.9402, -1.9237,  1.4471,\n",
              "          -1.6211, -0.4627,  0.0724, -2.5458,  0.3172,  0.7139,  0.8306,\n",
              "          -0.3575, -0.5453],\n",
              "         [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
              "           0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
              "          -0.3937,  0.5133],\n",
              "         [ 0.4887, -0.1694,  0.4623,  1.1920,  0.2188, -2.1144, -0.6303,\n",
              "          -1.2609, -0.4754, -0.4203, -0.3314,  0.0310,  0.8525,  0.0888,\n",
              "           0.4840, -0.7512],\n",
              "         [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
              "           0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
              "          -0.3937,  0.5133],\n",
              "         [ 1.0906, -0.5341,  1.5008, -2.1458, -0.2829,  0.4449, -2.3409,\n",
              "          -0.6489, -0.3310, -0.3682, -0.3702,  1.2048, -0.4190, -0.5210,\n",
              "          -0.8396, -1.5450]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first let's setup the residual connection that we'll use later\n",
        "h = x ## our embedding vectors that have NOT yet been positionally encodded\n",
        "print(f'h: {h.shape}\\n{h}')\n",
        "\n",
        "\n",
        "\n",
        "# now we'll perform our first normalization\n",
        "# first we square each entry in x and then take the mean of those values across each embedding vector\n",
        "mean_squared = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "mean_squared\n",
        "\n",
        "\n",
        "# then we multiply x by the reciprocal of the square roots of mean_squared\n",
        "# 1e-6 is a very small number added for stability just in case an entry happens to be equal to 0 (since you can't divide by 0)\n",
        "x_normed = x * torch.rsqrt(mean_squared + 1e-6)\n",
        "print(f'x_normed: {x_normed.shape}\\n{x_normed}')\n",
        "\n",
        "# and finally, we multiply by a learnable scale parameter\n",
        "# This scale is initialized to 1's but if we were to train then those values would change\n",
        "rms_scale = torch.ones(d)\n",
        "print(f'rms_scale: {rms_scale.shape}\\n{rms_scale}\\n')\n",
        "\n",
        "x_normed *= rms_scale\n",
        "print(f'x_normed: {x_normed.shape}\\n{x_normed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC2Uv3R79uc-",
        "outputId": "db17366e-76db-470d-e109-869e37eaaf5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h: torch.Size([1, 5, 16])\n",
            "tensor([[[ 0.7922, -1.0242,  0.1843, -1.0991, -0.9402, -1.9237,  1.4471,\n",
            "          -1.6211, -0.4627,  0.0724, -2.5458,  0.3172,  0.7139,  0.8306,\n",
            "          -0.3575, -0.5453],\n",
            "         [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
            "           0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
            "          -0.3937,  0.5133],\n",
            "         [ 0.4887, -0.1694,  0.4623,  1.1920,  0.2188, -2.1144, -0.6303,\n",
            "          -1.2609, -0.4754, -0.4203, -0.3314,  0.0310,  0.8525,  0.0888,\n",
            "           0.4840, -0.7512],\n",
            "         [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
            "           0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
            "          -0.3937,  0.5133],\n",
            "         [ 1.0906, -0.5341,  1.5008, -2.1458, -0.2829,  0.4449, -2.3409,\n",
            "          -0.6489, -0.3310, -0.3682, -0.3702,  1.2048, -0.4190, -0.5210,\n",
            "          -0.8396, -1.5450]]], grad_fn=<EmbeddingBackward0>)\n",
            "x_normed: torch.Size([1, 5, 16])\n",
            "tensor([[[ 0.6973, -0.9015,  0.1623, -0.9675, -0.8276, -1.6933,  1.2738,\n",
            "          -1.4269, -0.4073,  0.0637, -2.2408,  0.2792,  0.6284,  0.7311,\n",
            "          -0.3146, -0.4799],\n",
            "         [ 0.6188,  0.2337, -0.1037, -0.5242, -0.4345, -1.3270, -0.8352,\n",
            "           1.1873, -1.0162, -0.5597, -2.4325,  1.5821,  0.2529, -0.8691,\n",
            "          -0.4850,  0.6322],\n",
            "         [ 0.6046, -0.2096,  0.5719,  1.4747,  0.2707, -2.6157, -0.7797,\n",
            "          -1.5599, -0.5881, -0.5200, -0.4100,  0.0383,  1.0546,  0.1099,\n",
            "           0.5988, -0.9293],\n",
            "         [ 0.6188,  0.2337, -0.1037, -0.5242, -0.4345, -1.3270, -0.8352,\n",
            "           1.1873, -1.0162, -0.5597, -2.4325,  1.5821,  0.2529, -0.8691,\n",
            "          -0.4850,  0.6322],\n",
            "         [ 0.9783, -0.4791,  1.3463, -1.9249, -0.2538,  0.3991, -2.0999,\n",
            "          -0.5821, -0.2969, -0.3303, -0.3321,  1.0807, -0.3759, -0.4674,\n",
            "          -0.7532, -1.3860]]], grad_fn=<MulBackward0>)\n",
            "rms_scale: torch.Size([16])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "\n",
            "x_normed: torch.Size([1, 5, 16])\n",
            "tensor([[[ 0.6973, -0.9015,  0.1623, -0.9675, -0.8276, -1.6933,  1.2738,\n",
            "          -1.4269, -0.4073,  0.0637, -2.2408,  0.2792,  0.6284,  0.7311,\n",
            "          -0.3146, -0.4799],\n",
            "         [ 0.6188,  0.2337, -0.1037, -0.5242, -0.4345, -1.3270, -0.8352,\n",
            "           1.1873, -1.0162, -0.5597, -2.4325,  1.5821,  0.2529, -0.8691,\n",
            "          -0.4850,  0.6322],\n",
            "         [ 0.6046, -0.2096,  0.5719,  1.4747,  0.2707, -2.6157, -0.7797,\n",
            "          -1.5599, -0.5881, -0.5200, -0.4100,  0.0383,  1.0546,  0.1099,\n",
            "           0.5988, -0.9293],\n",
            "         [ 0.6188,  0.2337, -0.1037, -0.5242, -0.4345, -1.3270, -0.8352,\n",
            "           1.1873, -1.0162, -0.5597, -2.4325,  1.5821,  0.2529, -0.8691,\n",
            "          -0.4850,  0.6322],\n",
            "         [ 0.9783, -0.4791,  1.3463, -1.9249, -0.2538,  0.3991, -2.0999,\n",
            "          -0.5821, -0.2969, -0.3303, -0.3321,  1.0807, -0.3759, -0.4674,\n",
            "          -0.7532, -1.3860]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's turn that RMSNorm into a function that we'll be able to reuse repeatedly later\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ],
      "metadata": {
        "id": "NsanFB1Y-9Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Query Attention:\n",
        "\n",
        "<a id='f'></a>\n",
        "[multi-query attention](https://arxiv.org/abs/1911.02150) is the de facto standard for saving on parameter counts in order to get a bigger model.\n",
        "\n",
        "\n",
        "GPU's are computationally efficient, but not as good at storage. Using multi-Query attention we remove heads for V's and K's and only use multi-heads for Q's.\n",
        "\n",
        " The idea is that the model can make multiple queries to the residual state and have those many queries be answered by shared keys & values."
      ],
      "metadata": {
        "id": "uebZXYSAAK-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Why do we need normalization??**"
      ],
      "metadata": {
        "id": "onyw7DeLA3hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first up, remember we're currently working with two separate objects\n",
        "# x is for the residual connection and x_normed will go into our Attention calculation\n",
        "h, x_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnGse20mAIFC",
        "outputId": "ad24277c-9e19-43f6-a9f0-665e82ac6b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.7922, -1.0242,  0.1843, -1.0991, -0.9402, -1.9237,  1.4471,\n",
              "           -1.6211, -0.4627,  0.0724, -2.5458,  0.3172,  0.7139,  0.8306,\n",
              "           -0.3575, -0.5453],\n",
              "          [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
              "            0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
              "           -0.3937,  0.5133],\n",
              "          [ 0.4887, -0.1694,  0.4623,  1.1920,  0.2188, -2.1144, -0.6303,\n",
              "           -1.2609, -0.4754, -0.4203, -0.3314,  0.0310,  0.8525,  0.0888,\n",
              "            0.4840, -0.7512],\n",
              "          [ 0.5023,  0.1898, -0.0842, -0.4255, -0.3528, -1.0773, -0.6780,\n",
              "            0.9639, -0.8250, -0.4544, -1.9748,  1.2844,  0.2053, -0.7056,\n",
              "           -0.3937,  0.5133],\n",
              "          [ 1.0906, -0.5341,  1.5008, -2.1458, -0.2829,  0.4449, -2.3409,\n",
              "           -0.6489, -0.3310, -0.3682, -0.3702,  1.2048, -0.4190, -0.5210,\n",
              "           -0.8396, -1.5450]]], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([[[ 0.6973, -0.9015,  0.1623, -0.9675, -0.8276, -1.6933,  1.2738,\n",
              "           -1.4269, -0.4073,  0.0637, -2.2408,  0.2792,  0.6284,  0.7311,\n",
              "           -0.3146, -0.4799],\n",
              "          [ 0.6188,  0.2337, -0.1037, -0.5242, -0.4345, -1.3270, -0.8352,\n",
              "            1.1873, -1.0162, -0.5597, -2.4325,  1.5821,  0.2529, -0.8691,\n",
              "           -0.4850,  0.6322],\n",
              "          [ 0.6046, -0.2096,  0.5719,  1.4747,  0.2707, -2.6157, -0.7797,\n",
              "           -1.5599, -0.5881, -0.5200, -0.4100,  0.0383,  1.0546,  0.1099,\n",
              "            0.5988, -0.9293],\n",
              "          [ 0.6188,  0.2337, -0.1037, -0.5242, -0.4345, -1.3270, -0.8352,\n",
              "            1.1873, -1.0162, -0.5597, -2.4325,  1.5821,  0.2529, -0.8691,\n",
              "           -0.4850,  0.6322],\n",
              "          [ 0.9783, -0.4791,  1.3463, -1.9249, -0.2538,  0.3991, -2.0999,\n",
              "           -0.5821, -0.2969, -0.3303, -0.3321,  1.0807, -0.3759, -0.4674,\n",
              "           -0.7532, -1.3860]]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the hyperparameters of MQA\n",
        "\n",
        "num_kv_heads = 2 # Llama uses 8 key and value heads per layer\n",
        "assert num_heads % num_kv_heads == 0 # each query needs to match up to a kv so checking for perfect divisibility\n",
        "print(f\"as a reminder: num_heads = {num_heads}, head_dim = {head_dim}\")\n",
        "\n",
        "\n",
        "## nn.Linear(): This creates a linear layer, which applies a linear transformation to the input data.\n",
        "## d: This is the input dimension of the linear layer, meaning the layer expects input tensors to have d = 16 features.\n",
        "## (num_heads * head_dim) : This is the output dimension of the linear layer. It calculates the total number of output features by multiplying the number of attention heads (num_heads) by the dimension of each head (head_dim).\n",
        "## bias=False: This argument indicates that the linear layer should not have a bias term.\n",
        "\n",
        "# now we'll initialize our self-attention Weight Matrices\n",
        "wq = nn.Linear(d, num_heads * head_dim, bias=False)\n",
        "wk = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
        "wv = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
        "print(\"Attention weights: \", wq.weight.shape, wk.weight.shape, wv.weight.shape)\n",
        "\n",
        "# and project x_normed out to get our queries, keys and values\n",
        "xq = wq(x_normed)\n",
        "xk = wk(x_normed)\n",
        "xv = wv(x_normed)\n",
        "print(\"Attention projections: \", xq.shape, xk.shape, xv.shape)\n",
        "\n",
        "# then reshape them to separate out by head\n",
        "xq = xq.view(b, seq_len, num_heads, head_dim)\n",
        "xk = xk.view(b, seq_len, num_kv_heads, head_dim)\n",
        "xv = xv.view(b, seq_len, num_kv_heads, head_dim)\n",
        "print(\"Reshaped: \", xq.shape, xk.shape, xv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tka2wTDAA05A",
        "outputId": "66ab6d58-1414-4508-c8fd-e94dece5a36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "as a reminder: num_heads = 4, head_dim = 4\n",
            "Attention weights:  torch.Size([16, 16]) torch.Size([8, 16]) torch.Size([8, 16])\n",
            "Attention projections:  torch.Size([1, 5, 16]) torch.Size([1, 5, 8]) torch.Size([1, 5, 8])\n",
            "Reshaped:  torch.Size([1, 5, 4, 4]) torch.Size([1, 5, 2, 4]) torch.Size([1, 5, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1g. Apply Pre-Computed RoPE Positional Encodings:\n",
        "<a id='g'></a>\n",
        "\n",
        "Earlier we pre-computed the frequencies for rotation. Now we'll actually apply our rotary embeddings."
      ],
      "metadata": {
        "id": "4hPehtChEbTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we reshape and then view our queries and keys as complex values, the type of number that works well with rotation\n",
        "xq = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "xk = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
        "print(f'xk: {xk.shape}\\n{xk}')\n",
        "\n",
        "\n",
        "ndim = xq.ndim\n",
        "assert 0 <= 1 < ndim\n",
        "assert freqs_cis.shape == (xq.shape[1], xq.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != xq.shape[1], xq.shape[-1] {(xq.shape[1], xq.shape[-1])}'\n",
        "\n",
        "# reshape our queries\n",
        "shape = [d if i == 1 or i == xq.ndim - 1 else 1 for i, d in enumerate(xq.shape)]\n",
        "print(f'shape: {shape}\\n')\n",
        "\n",
        "freqs_cis = freqs_cis.view(*shape)\n",
        "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')\n",
        "\n",
        "# now multiply the data by the frequencies, turn them back into real numbers, revert the shape and make sure they're of the right type\n",
        "xq = torch.view_as_real(xq * freqs_cis).flatten(3).type_as(xv)\n",
        "xk = torch.view_as_real(xk * freqs_cis).flatten(3).type_as(xv)\n",
        "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
        "print(f'xk: {xk.shape}\\n{xk}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se9xc_jiBaRl",
        "outputId": "8edec0dd-5c14-4b3b-c1ae-5287e2602fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq: torch.Size([1, 5, 4, 2])\n",
            "tensor([[[[-1.0814-1.2398j, -0.8482-0.3291j],\n",
            "          [ 0.0121-1.7533j,  0.2948-0.1679j],\n",
            "          [ 0.9378+0.1311j,  0.4642-0.0390j],\n",
            "          [-0.2767-0.9907j,  0.1970+0.5204j]],\n",
            "\n",
            "         [[ 0.1150+0.2663j, -0.2363-0.5226j],\n",
            "          [ 0.8432-0.4822j,  1.5463-0.9210j],\n",
            "          [ 0.6538-0.0643j,  0.5585-0.4385j],\n",
            "          [-0.0605-0.4828j,  0.1684+0.1511j]],\n",
            "\n",
            "         [[-0.1797-0.2311j, -0.6211-0.4716j],\n",
            "          [ 0.8583-0.6888j, -0.1591-1.0148j],\n",
            "          [ 0.7652-0.3836j,  0.2293-0.0237j],\n",
            "          [-1.1040-1.0531j,  0.2276-0.1342j]],\n",
            "\n",
            "         [[ 0.1150+0.2663j, -0.2363-0.5226j],\n",
            "          [ 0.8432-0.4822j,  1.5463-0.9210j],\n",
            "          [ 0.6538-0.0643j,  0.5585-0.4385j],\n",
            "          [-0.0605-0.4828j,  0.1684+0.1511j]],\n",
            "\n",
            "         [[ 0.4101-0.3572j,  0.4347-0.5412j],\n",
            "          [ 0.8725-1.0928j,  0.0247-0.2400j],\n",
            "          [-0.4210+0.0369j,  0.6348-0.8082j],\n",
            "          [-0.3813-1.1089j, -0.1655+0.0956j]]]],\n",
            "       grad_fn=<ViewAsComplexBackward0>)\n",
            "\n",
            "xk: torch.Size([1, 5, 2, 2])\n",
            "tensor([[[[ 0.6295+0.5556j,  0.9096+0.1975j],\n",
            "          [ 0.0104-0.9554j,  0.5539+0.8921j]],\n",
            "\n",
            "         [[ 0.0145-0.0851j,  1.0444+0.3055j],\n",
            "          [-0.1932+0.2795j,  0.2935+0.1660j]],\n",
            "\n",
            "         [[ 0.8416+0.7438j,  0.9321+0.1645j],\n",
            "          [ 0.3086+0.2881j,  0.7085+0.2632j]],\n",
            "\n",
            "         [[ 0.0145-0.0851j,  1.0444+0.3055j],\n",
            "          [-0.1932+0.2795j,  0.2935+0.1660j]],\n",
            "\n",
            "         [[ 0.3040-0.2975j,  0.8074-0.3439j],\n",
            "          [ 0.0900-0.1904j,  0.3553+0.7827j]]]],\n",
            "       grad_fn=<ViewAsComplexBackward0>)\n",
            "shape: [1, 5, 1, 2]\n",
            "\n",
            "freqs_cis: torch.Size([1, 5, 1, 2])\n",
            "tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j]],\n",
            "\n",
            "         [[ 0.5403+0.8415j,  0.9999+0.0100j]],\n",
            "\n",
            "         [[-0.4161+0.9093j,  0.9998+0.0200j]],\n",
            "\n",
            "         [[-0.9900+0.1411j,  0.9996+0.0300j]],\n",
            "\n",
            "         [[-0.6536-0.7568j,  0.9992+0.0400j]]]])\n",
            "xq: torch.Size([1, 5, 4, 4])\n",
            "tensor([[[[-1.0814, -1.2398, -0.8482, -0.3291],\n",
            "          [ 0.0121, -1.7533,  0.2948, -0.1679],\n",
            "          [ 0.9378,  0.1311,  0.4642, -0.0390],\n",
            "          [-0.2767, -0.9907,  0.1970,  0.5204]],\n",
            "\n",
            "         [[-0.1619,  0.2406, -0.2311, -0.5250],\n",
            "          [ 0.8613,  0.4490,  1.5554, -0.9054],\n",
            "          [ 0.4074,  0.5154,  0.5628, -0.4329],\n",
            "          [ 0.3736, -0.3117,  0.1669,  0.1528]],\n",
            "\n",
            "         [[ 0.2849, -0.0672, -0.6115, -0.4839],\n",
            "          [ 0.2691,  1.0671, -0.1387, -1.0178],\n",
            "          [ 0.0304,  0.8555,  0.2297, -0.0191],\n",
            "          [ 1.4170, -0.5656,  0.2302, -0.1296]],\n",
            "\n",
            "         [[-0.1514, -0.2474, -0.2205, -0.5295],\n",
            "          [-0.7667,  0.5963,  1.5732, -0.8742],\n",
            "          [-0.6381,  0.1559,  0.5714, -0.4216],\n",
            "          [ 0.1280,  0.4694,  0.1638,  0.1561]],\n",
            "\n",
            "         [[-0.5384, -0.0769,  0.4560, -0.5234],\n",
            "          [-1.3974,  0.0539,  0.0343, -0.2388],\n",
            "          [ 0.3031,  0.2945,  0.6667, -0.7821],\n",
            "          [-0.5900,  1.0134, -0.1692,  0.0889]]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "xk: torch.Size([1, 5, 2, 4])\n",
            "tensor([[[[ 0.6295,  0.5556,  0.9096,  0.1975],\n",
            "          [ 0.0104, -0.9554,  0.5539,  0.8921]],\n",
            "\n",
            "         [[ 0.0795, -0.0338,  1.0413,  0.3160],\n",
            "          [-0.3396, -0.0115,  0.2919,  0.1689]],\n",
            "\n",
            "         [[-1.0265,  0.4557,  0.9286,  0.1831],\n",
            "          [-0.3904,  0.1607,  0.7031,  0.2773]],\n",
            "\n",
            "         [[-0.0024,  0.0863,  1.0348,  0.3367],\n",
            "          [ 0.1518, -0.3040,  0.2884,  0.1747]],\n",
            "\n",
            "         [[-0.4238, -0.0356,  0.8205, -0.3113],\n",
            "          [-0.2029,  0.0563,  0.3237,  0.7963]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Calculating Self-Attention:\n",
        "\n"
      ],
      "metadata": {
        "id": "hoFMQtsLIT1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the number of K & V heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
        "if num_kv_heads != num_heads:\n",
        "  num_queries_per_kv = num_heads // num_kv_heads\n",
        "  xk = torch.repeat_interleave(xk, num_queries_per_kv, dim=2)\n",
        "  xv = torch.repeat_interleave(xv, num_queries_per_kv, dim=2)\n",
        "\n",
        "xq.shape, xk.shape, xv.shape\n",
        "\n",
        "\n",
        "# Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
        "xq = xq.transpose(1, 2)\n",
        "xk = xk.transpose(1, 2)\n",
        "xv = xv.transpose(1, 2)\n",
        "\n",
        "xq.shape, xk.shape, xv.shape\n",
        "\n",
        "\n",
        "# Calculates attention logits by performing a batch matrix multiplication between queries and keys\n",
        "scores = torch.matmul(xq, xk.transpose(2, 3))\n",
        "\n",
        "# then we scale the logits by the reciprocal of the square root of the head dimension\n",
        "scores = scores / math.sqrt(head_dim)\n",
        "\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utr6pJodFDC8",
        "outputId": "d632653e-2714-4206-e8d3-5467a4a2cd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 5]),\n",
              " tensor([[[[-1.1030, -0.5156, -0.1514, -0.5465, -0.0455],\n",
              "           [-0.1411, -0.2138, -0.0174, -0.1974,  0.0170],\n",
              "           [-0.2549, -0.3824, -0.4898, -0.4011, -0.2347],\n",
              "           [-0.2690, -0.2003, -0.1295, -0.2138,  0.0284],\n",
              "           [-0.0351,  0.1346,  0.4226,  0.1451,  0.3840]],\n",
              " \n",
              "          [[-0.3657,  0.1571, -0.2842,  0.0486,  0.1757],\n",
              "           [ 1.0138,  0.6935,  0.2995,  0.6707,  0.5885],\n",
              "           [ 0.2175, -0.2403, -0.0526, -0.1974,  0.0255],\n",
              "           [ 0.5534,  0.6405,  1.1798,  0.6935,  0.9333],\n",
              "           [-0.4328, -0.0763,  0.7236, -0.0185,  0.3464]],\n",
              " \n",
              "          [[ 0.0534, -0.0956, -0.0147,  0.1148, -0.0319],\n",
              "           [-0.2813, -0.0266,  0.0997, -0.0041, -0.1081],\n",
              "           [-0.3534,  0.0218,  0.1409, -0.0963,  0.0506],\n",
              "           [-0.1076,  0.1552,  0.2795, -0.0266, -0.0063],\n",
              "           [-0.3034, -0.0219,  0.0904,  0.0060, -0.2260]],\n",
              " \n",
              "          [[ 0.7585,  0.1254,  0.1158,  0.2035,  0.2392],\n",
              "           [ 0.2652, -0.0244, -0.0181,  0.1132,  0.0412],\n",
              "           [ 0.2835, -0.2147, -0.2591,  0.2154, -0.1740],\n",
              "           [-0.1086,  0.0126,  0.0920, -0.0244,  0.0889],\n",
              "           [-0.4944,  0.0772,  0.1495, -0.2154,  0.0964]]]],\n",
              "        grad_fn=<DivBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Mask we created earlier:"
      ],
      "metadata": {
        "id": "m6Gg4Mj4J1wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we get to use the mask that we precomputed earlier\n",
        "scores = scores + mask\n",
        "\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfpn5X4sIbu1",
        "outputId": "00554389-939a-41da-f884-fc985ed98029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 5]),\n",
              " tensor([[[[-1.1030,    -inf,    -inf,    -inf,    -inf],\n",
              "           [-0.1411, -0.2138,    -inf,    -inf,    -inf],\n",
              "           [-0.2549, -0.3824, -0.4898,    -inf,    -inf],\n",
              "           [-0.2690, -0.2003, -0.1295, -0.2138,    -inf],\n",
              "           [-0.0351,  0.1346,  0.4226,  0.1451,  0.3840]],\n",
              " \n",
              "          [[-0.3657,    -inf,    -inf,    -inf,    -inf],\n",
              "           [ 1.0138,  0.6935,    -inf,    -inf,    -inf],\n",
              "           [ 0.2175, -0.2403, -0.0526,    -inf,    -inf],\n",
              "           [ 0.5534,  0.6405,  1.1798,  0.6935,    -inf],\n",
              "           [-0.4328, -0.0763,  0.7236, -0.0185,  0.3464]],\n",
              " \n",
              "          [[ 0.0534,    -inf,    -inf,    -inf,    -inf],\n",
              "           [-0.2813, -0.0266,    -inf,    -inf,    -inf],\n",
              "           [-0.3534,  0.0218,  0.1409,    -inf,    -inf],\n",
              "           [-0.1076,  0.1552,  0.2795, -0.0266,    -inf],\n",
              "           [-0.3034, -0.0219,  0.0904,  0.0060, -0.2260]],\n",
              " \n",
              "          [[ 0.7585,    -inf,    -inf,    -inf,    -inf],\n",
              "           [ 0.2652, -0.0244,    -inf,    -inf,    -inf],\n",
              "           [ 0.2835, -0.2147, -0.2591,    -inf,    -inf],\n",
              "           [-0.1086,  0.0126,  0.0920, -0.0244,    -inf],\n",
              "           [-0.4944,  0.0772,  0.1495, -0.2154,  0.0964]]]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we perform the softmax operation to get our actual probabilities\n",
        "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "scores\n",
        "# notice that thanks to the causal mask, 0 probability is placed on future tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP3VIFJjJ7ft",
        "outputId": "07f96935-5d36-4dc8-e070-4185e532b1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5182, 0.4818, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3744, 0.3296, 0.2960, 0.0000, 0.0000],\n",
              "          [0.2338, 0.2504, 0.2688, 0.2471, 0.0000],\n",
              "          [0.1542, 0.1828, 0.2438, 0.1847, 0.2345]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5794, 0.4206, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4174, 0.2640, 0.3186, 0.0000, 0.0000],\n",
              "          [0.1956, 0.2134, 0.3660, 0.2250, 0.0000],\n",
              "          [0.1075, 0.1536, 0.3418, 0.1627, 0.2344]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4367, 0.5633, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2442, 0.3554, 0.4004, 0.0000, 0.0000],\n",
              "          [0.2059, 0.2677, 0.3032, 0.2232, 0.0000],\n",
              "          [0.1600, 0.2120, 0.2372, 0.2180, 0.1729]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5719, 0.4281, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4569, 0.2776, 0.2655, 0.0000, 0.0000],\n",
              "          [0.2253, 0.2543, 0.2753, 0.2451, 0.0000],\n",
              "          [0.1282, 0.2270, 0.2440, 0.1694, 0.2314]]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then matmul by our values projection\n",
        "output = torch.matmul(scores, xv)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhT_G41nKNI6",
        "outputId": "3e9c8bb6-0d69-4c19-ac1e-2e2a2e8a4285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 4]),\n",
              " tensor([[[[-0.3411, -0.2834,  0.7208,  0.2801],\n",
              "           [-0.6501, -0.7035,  0.7213,  0.7681],\n",
              "           [-0.6010, -0.7171,  0.5341,  0.6377],\n",
              "           [-0.7042, -0.8500,  0.5515,  0.8055],\n",
              "           [-0.5821, -0.7557,  0.4780,  0.8053]],\n",
              " \n",
              "          [[-0.3411, -0.2834,  0.7208,  0.2801],\n",
              "           [-0.6108, -0.6501,  0.7213,  0.7061],\n",
              "           [-0.5627, -0.6711,  0.5198,  0.5731],\n",
              "           [-0.6823, -0.8466,  0.4901,  0.7535],\n",
              "           [-0.5654, -0.7596,  0.4161,  0.7613]],\n",
              " \n",
              "          [[ 0.8611,  0.4087,  1.4048,  0.7438],\n",
              "           [ 0.6380,  0.3079,  0.6681,  0.8952],\n",
              "           [ 0.2859,  0.0231,  0.5375,  0.9409],\n",
              "           [ 0.3377,  0.0770,  0.4579,  0.9527],\n",
              "           [ 0.2379, -0.0508,  0.4032,  0.7980]],\n",
              " \n",
              "          [[ 0.8611,  0.4087,  1.4048,  0.7438],\n",
              "           [ 0.6916,  0.3321,  0.8449,  0.8588],\n",
              "           [ 0.4630,  0.1454,  0.7748,  0.8858],\n",
              "           [ 0.3646,  0.0979,  0.4749,  0.9479],\n",
              "           [ 0.1775, -0.1153,  0.3722,  0.7496]]]],\n",
              "        grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and reshape to put the sequence length back into place and the outputs of our heads lined up\n",
        "output = output.transpose(1, 2).contiguous().view(b, seq_len, -1)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPKu_J3ZKUSX",
        "outputId": "1d4f94cc-9b0e-4959-93db-33a9bcb580c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[-0.3411, -0.2834,  0.7208,  0.2801, -0.3411, -0.2834,  0.7208,\n",
              "            0.2801,  0.8611,  0.4087,  1.4048,  0.7438,  0.8611,  0.4087,\n",
              "            1.4048,  0.7438],\n",
              "          [-0.6501, -0.7035,  0.7213,  0.7681, -0.6108, -0.6501,  0.7213,\n",
              "            0.7061,  0.6380,  0.3079,  0.6681,  0.8952,  0.6916,  0.3321,\n",
              "            0.8449,  0.8588],\n",
              "          [-0.6010, -0.7171,  0.5341,  0.6377, -0.5627, -0.6711,  0.5198,\n",
              "            0.5731,  0.2859,  0.0231,  0.5375,  0.9409,  0.4630,  0.1454,\n",
              "            0.7748,  0.8858],\n",
              "          [-0.7042, -0.8500,  0.5515,  0.8055, -0.6823, -0.8466,  0.4901,\n",
              "            0.7535,  0.3377,  0.0770,  0.4579,  0.9527,  0.3646,  0.0979,\n",
              "            0.4749,  0.9479],\n",
              "          [-0.5821, -0.7557,  0.4780,  0.8053, -0.5654, -0.7596,  0.4161,\n",
              "            0.7613,  0.2379, -0.0508,  0.4032,  0.7980,  0.1775, -0.1153,\n",
              "            0.3722,  0.7496]]], grad_fn=<ViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we can initialize and apply our output projection that mixes the information from the heads together\n",
        "wo = nn.Linear(num_heads * head_dim, d, bias=False)\n",
        "Xout = wo(output)\n",
        "Xout.shape, Xout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jlf34iSKZ0k",
        "outputId": "83cb4368-fa39-4b32-da05-ab69e0f6f092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 0.1081,  0.1188, -0.5131,  0.0919,  0.1291,  0.3527,  0.2897,\n",
              "            0.0015,  0.4972,  0.4286, -0.1191,  0.3706, -0.2346, -0.0786,\n",
              "           -0.2442, -0.5780],\n",
              "          [ 0.3537,  0.4496, -0.4476,  0.2291,  0.0298,  0.0955, -0.1814,\n",
              "           -0.1036,  0.5948,  0.2303, -0.1553,  0.7117, -0.0258, -0.0470,\n",
              "           -0.2564, -1.0106],\n",
              "          [ 0.3688,  0.5131, -0.4693,  0.1576, -0.0749, -0.0713, -0.1770,\n",
              "           -0.0439,  0.4747,  0.1959, -0.1184,  0.7113,  0.0079,  0.0852,\n",
              "           -0.1823, -0.9736],\n",
              "          [ 0.4203,  0.5579, -0.4546,  0.2267, -0.0476, -0.0901, -0.2981,\n",
              "           -0.0890,  0.5570,  0.1949, -0.1651,  0.7934,  0.0639,  0.1002,\n",
              "           -0.1195, -1.2026],\n",
              "          [ 0.3947,  0.4520, -0.4219,  0.1850, -0.0317, -0.1611, -0.2762,\n",
              "           -0.0674,  0.4483,  0.1784, -0.1560,  0.6925,  0.0200,  0.0808,\n",
              "           -0.0115, -1.0943]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1i. Our first residual connection\n",
        "<a id='i'></a>\n",
        "Here we'll normalize the output of our attention mechanism and then add it to our residual state\n",
        "\n",
        "# What is residual state?"
      ],
      "metadata": {
        "id": "6fjtSNdsT9up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h += Xout\n",
        "h.shape, h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9RxJh_HKd3M",
        "outputId": "9c3e7f53-3268-493c-da26-a99c3b30a293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 9.0023e-01, -9.0545e-01, -3.2878e-01, -1.0072e+00, -8.1104e-01,\n",
              "           -1.5710e+00,  1.7369e+00, -1.6196e+00,  3.4522e-02,  5.0097e-01,\n",
              "           -2.6649e+00,  6.8783e-01,  4.7931e-01,  7.5197e-01, -6.0171e-01,\n",
              "           -1.1232e+00],\n",
              "          [ 8.5605e-01,  6.3933e-01, -5.3178e-01, -1.9641e-01, -3.2296e-01,\n",
              "           -9.8177e-01, -8.5948e-01,  8.6021e-01, -2.3014e-01, -2.2408e-01,\n",
              "           -2.1301e+00,  1.9960e+00,  1.7949e-01, -7.5255e-01, -6.5012e-01,\n",
              "           -4.9730e-01],\n",
              "          [ 8.5751e-01,  3.4367e-01, -7.0630e-03,  1.3496e+00,  1.4391e-01,\n",
              "           -2.1857e+00, -8.0728e-01, -1.3049e+00, -7.5397e-04, -2.2443e-01,\n",
              "           -4.4976e-01,  7.4226e-01,  8.6041e-01,  1.7402e-01,  3.0177e-01,\n",
              "           -1.7248e+00],\n",
              "          [ 9.2261e-01,  7.4765e-01, -5.3878e-01, -1.9884e-01, -4.0034e-01,\n",
              "           -1.1674e+00, -9.7616e-01,  8.7485e-01, -2.6800e-01, -2.5943e-01,\n",
              "           -2.1399e+00,  2.0778e+00,  2.6921e-01, -6.0532e-01, -5.1326e-01,\n",
              "           -6.8935e-01],\n",
              "          [ 1.4853e+00, -8.2086e-02,  1.0788e+00, -1.9608e+00, -3.1462e-01,\n",
              "            2.8384e-01, -2.6171e+00, -7.1631e-01,  1.1729e-01, -1.8980e-01,\n",
              "           -5.2625e-01,  1.8973e+00, -3.9897e-01, -4.4025e-01, -8.5115e-01,\n",
              "           -2.6393e+00]]], grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then we'll normalize the current state of our residual for use in our MoE later\n",
        "pre_ffwd_norm = RMSNorm(d)\n",
        "h_normed = pre_ffwd_norm(h)\n",
        "# so now we're working with x, which we'll use later for our next residual conenction, and x_normed which is used by our MoE MLP"
      ],
      "metadata": {
        "id": "0gMWRbAhUHrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1j. The SwiGLU Feedforward Network\n",
        "<a id='j'></a>\n",
        "\n",
        "Llama models have surprisingly not opted for a mixture of experts strategy which i was assuming they'd go for by now. Their feedforward networks use the SwiGLU activation which basically uses the activation function as a gate that dynamically determines what information gets through"
      ],
      "metadata": {
        "id": "EUxlNl_nUbDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we need to define our actual hidden dimension, which Llama's code does in an unnecessarily complicated manner\n",
        "hidden_dim = 4 * d # usually i would designate a hyperparameter for this 4, but in llama's code it was just there\n",
        "print(hidden_dim)\n",
        "hidden_dim = int(2 * hidden_dim / 3)\n",
        "print(hidden_dim)\n",
        "multiple_of = 256 # their description of this was \"make SwiGLU hidden layer size multiple of large power of 2\"\n",
        "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "print(hidden_dim)\n",
        "# so basically this overly convoluted setup is designed to ensure that hidden_dim is a multiple of 256, likely for hardware efficiency reasons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMu7dInyUNSf",
        "outputId": "37a8a02c-6bd6-4e96-b5a0-07bd19336229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "42\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "up = nn.Linear(d, hidden_dim, bias=False)\n",
        "gate = nn.Linear(d, hidden_dim, bias=False)\n",
        "down = nn.Linear(hidden_dim, d, bias=False)\n",
        "\n",
        "up_proj = up(h_normed)\n",
        "print(up_proj.shape, up_proj)\n",
        "\n",
        "gate_proj = F.silu(gate(h_normed))\n",
        "print(gate_proj.shape, gate_proj)\n",
        "\n",
        "ffwd_output = down(up_proj * gate_proj)\n",
        "print(ffwd_output.shape, ffwd_output)\n",
        "\n",
        "# and then do our final residual connection of this layer\n",
        "out = h + ffwd_output\n",
        "print(out.shape, out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ojJmoOUg0Y",
        "outputId": "445dcb91-8c4d-4c14-b465-b937001fbd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 256]) tensor([[[-0.0257, -0.4757,  0.2491,  ..., -0.3339, -0.1644, -0.6306],\n",
            "         [-0.1191,  0.1224, -0.4527,  ..., -0.6046, -0.8389,  0.7007],\n",
            "         [ 0.0750, -0.1991,  0.2303,  ..., -0.3422,  0.0081,  0.2349],\n",
            "         [-0.1029,  0.1211, -0.4618,  ..., -0.5956, -0.7935,  0.6934],\n",
            "         [-0.1519, -0.3932, -0.9028,  ..., -0.0702, -0.6066, -0.2438]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "torch.Size([1, 5, 256]) tensor([[[-0.1094,  0.3486, -0.2633,  ..., -0.0676,  0.1021, -0.1896],\n",
            "         [ 0.0761, -0.0480, -0.2195,  ...,  0.0683, -0.1498, -0.1238],\n",
            "         [-0.1262, -0.1389, -0.2595,  ..., -0.1223, -0.0722,  0.2790],\n",
            "         [ 0.0541, -0.0701, -0.2385,  ...,  0.0567, -0.1545, -0.1101],\n",
            "         [ 1.1919, -0.2752, -0.2087,  ..., -0.1679, -0.2504,  0.2954]]],\n",
            "       grad_fn=<SiluBackward0>)\n",
            "torch.Size([1, 5, 16]) tensor([[[ 0.1607,  0.0911, -0.1415,  0.0118,  0.0690,  0.1104,  0.0337,\n",
            "           0.0371,  0.0367,  0.0695,  0.0220, -0.1304,  0.0840,  0.0495,\n",
            "           0.0759, -0.1186],\n",
            "         [-0.0859,  0.0892,  0.0205,  0.1056, -0.0004, -0.1990,  0.1385,\n",
            "           0.0596,  0.0416,  0.0416, -0.1645, -0.0413, -0.1139,  0.0339,\n",
            "          -0.0804, -0.0766],\n",
            "         [-0.0413,  0.0521,  0.0565,  0.1413,  0.0602, -0.0328,  0.0908,\n",
            "          -0.0334,  0.0181, -0.0109, -0.0333, -0.1778,  0.0017,  0.0390,\n",
            "           0.1133,  0.0662],\n",
            "         [-0.1002,  0.1170,  0.0120,  0.1037,  0.0042, -0.2107,  0.1307,\n",
            "           0.0392,  0.0458,  0.0438, -0.1569, -0.0472, -0.1156,  0.0559,\n",
            "          -0.0759, -0.0680],\n",
            "         [-0.0596,  0.1111,  0.0423,  0.0628,  0.1510,  0.0051, -0.0943,\n",
            "          -0.0508, -0.0681, -0.0819, -0.0554, -0.0174,  0.1298,  0.1561,\n",
            "           0.0419,  0.0768]]], grad_fn=<UnsafeViewBackward0>)\n",
            "torch.Size([1, 5, 16]) tensor([[[ 1.0610, -0.8144, -0.4703, -0.9953, -0.7420, -1.4606,  1.7706,\n",
            "          -1.5825,  0.0712,  0.5705, -2.6429,  0.5574,  0.5633,  0.8015,\n",
            "          -0.5258, -1.2418],\n",
            "         [ 0.7701,  0.7285, -0.5113, -0.0908, -0.3234, -1.1808, -0.7210,\n",
            "           0.9198, -0.1885, -0.1825, -2.2946,  1.9548,  0.0656, -0.7187,\n",
            "          -0.7305, -0.5739],\n",
            "         [ 0.8162,  0.3958,  0.0494,  1.4909,  0.2041, -2.2185, -0.7164,\n",
            "          -1.3383,  0.0173, -0.2353, -0.4831,  0.5645,  0.8621,  0.2130,\n",
            "           0.4151, -1.6586],\n",
            "         [ 0.8224,  0.8646, -0.5267, -0.0952, -0.3961, -1.3781, -0.8455,\n",
            "           0.9140, -0.2222, -0.2157, -2.2968,  2.0305,  0.1536, -0.5494,\n",
            "          -0.5891, -0.7573],\n",
            "         [ 1.4257,  0.0290,  1.1212, -1.8980, -0.1636,  0.2890, -2.7114,\n",
            "          -0.7671,  0.0491, -0.2717, -0.5816,  1.8799, -0.2692, -0.2841,\n",
            "          -0.8093, -2.5625]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output:\n",
        "\n",
        "So usually we'd run it back on steps 1e through 1j for however many layers our model has (Llama 3 8b uses 32) using different weight matrices but you get the point.\n",
        "\n",
        "\n",
        "Since our current `out` is of the same shape that it would be if we were to do more layers, let's go ahead and just see what Llama's output mechanism looks like. It's nothing interesting though, just a linear layer. Notably they chose to use a separate linear layer rather than re-using the embedding layer as is relatively common"
      ],
      "metadata": {
        "id": "F4zpMOxGWWJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we norm the residual state\n",
        "final_norm = RMSNorm(d)\n",
        "out_normed = final_norm(out)"
      ],
      "metadata": {
        "id": "c6Yp7jyrVzo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then multiply by the linear layer to get our final output logits\n",
        "final_output = nn.Linear(d, v, bias=False)\n",
        "logits = final_output(out_normed).float()\n",
        "logits.shape, logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw6_n5B7Wkwd",
        "outputId": "1e3d111d-50e5-4212-9bd1-73a81626e493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 10]),\n",
              " tensor([[[ 0.3093,  0.6462, -0.8326, -0.5472,  0.2947, -0.7590,  0.6102,\n",
              "           -1.3271,  0.0114, -0.1131],\n",
              "          [-0.7030,  0.2899, -0.0378,  0.1744,  0.7107, -0.4401,  0.2596,\n",
              "           -0.6487, -0.0036,  1.3435],\n",
              "          [-0.8501, -0.3519,  0.0763, -0.6963,  0.4976, -0.6761,  0.5076,\n",
              "           -1.2458,  0.7464,  0.8849],\n",
              "          [-0.7211,  0.2250, -0.0505,  0.1692,  0.7335, -0.4843,  0.2953,\n",
              "           -0.7043,  0.0515,  1.3766],\n",
              "          [-0.5662, -0.1354, -0.0931,  0.6919,  0.0322, -0.4636,  0.1797,\n",
              "           -0.1944, -0.2474,  0.5938]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax the logits to get the probability for each token's prediction across every token in the sequence\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w66Mrcj-WndZ",
        "outputId": "485ee200-6432-4ac7-a0bc-262a884bc880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1348, 0.1888, 0.0430, 0.0573, 0.1329, 0.0463, 0.1822, 0.0262,\n",
              "          0.1001, 0.0884],\n",
              "         [0.0372, 0.1004, 0.0723, 0.0894, 0.1529, 0.0484, 0.0974, 0.0393,\n",
              "          0.0749, 0.2879],\n",
              "         [0.0377, 0.0620, 0.0951, 0.0439, 0.1450, 0.0448, 0.1465, 0.0254,\n",
              "          0.1860, 0.2136],\n",
              "         [0.0362, 0.0933, 0.0708, 0.0882, 0.1551, 0.0459, 0.1001, 0.0368,\n",
              "          0.0784, 0.2951],\n",
              "         [0.0534, 0.0822, 0.0858, 0.1880, 0.0972, 0.0592, 0.1127, 0.0775,\n",
              "          0.0735, 0.1705]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedily decode the probabilities to get our final predicted indices\n",
        "greedy_indices = torch.argmax(probs, dim=-1)\n",
        "greedy_indices\n",
        "# if we were performing inference rather than training, that final token in the list would be the one to show the user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s_IzTQ4WpkE",
        "outputId": "e2055d46-78ec-4004-cd03-e0062b972c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 9, 9, 9, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKdLVSmEWuNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}